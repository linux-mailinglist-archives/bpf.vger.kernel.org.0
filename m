Return-Path: <bpf+bounces-8546-lists+bpf=lfdr.de@vger.kernel.org>
X-Original-To: lists+bpf@lfdr.de
Delivered-To: lists+bpf@lfdr.de
Received: from ny.mirrors.kernel.org (ny.mirrors.kernel.org [147.75.199.223])
	by mail.lfdr.de (Postfix) with ESMTPS id 018AC7880EB
	for <lists+bpf@lfdr.de>; Fri, 25 Aug 2023 09:35:04 +0200 (CEST)
Received: from smtp.subspace.kernel.org (wormhole.subspace.kernel.org [52.25.139.140])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by ny.mirrors.kernel.org (Postfix) with ESMTPS id D1D6C1C20F5C
	for <lists+bpf@lfdr.de>; Fri, 25 Aug 2023 07:35:02 +0000 (UTC)
Received: from localhost.localdomain (localhost.localdomain [127.0.0.1])
	by smtp.subspace.kernel.org (Postfix) with ESMTP id A49A11C14;
	Fri, 25 Aug 2023 07:34:53 +0000 (UTC)
X-Original-To: bpf@vger.kernel.org
Received: from lindbergh.monkeyblade.net (lindbergh.monkeyblade.net [23.128.96.19])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by smtp.subspace.kernel.org (Postfix) with ESMTPS id 3A0051C01
	for <bpf@vger.kernel.org>; Fri, 25 Aug 2023 07:34:52 +0000 (UTC)
Received: from szxga02-in.huawei.com (szxga02-in.huawei.com [45.249.212.188])
	by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 46DDF1FD9;
	Fri, 25 Aug 2023 00:34:50 -0700 (PDT)
Received: from kwepemi500020.china.huawei.com (unknown [172.30.72.53])
	by szxga02-in.huawei.com (SkyGuard) with ESMTP id 4RXBVh3N95zVjN8;
	Fri, 25 Aug 2023 15:32:28 +0800 (CST)
Received: from [10.67.109.184] (10.67.109.184) by
 kwepemi500020.china.huawei.com (7.221.188.8) with Microsoft SMTP Server
 (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256) id
 15.1.2507.31; Fri, 25 Aug 2023 15:34:46 +0800
Message-ID: <b4d5aaaf-7fe6-29fd-645a-62a4032820ae@huawei.com>
Date: Fri, 25 Aug 2023 15:34:45 +0800
Precedence: bulk
X-Mailing-List: bpf@vger.kernel.org
List-Id: <bpf.vger.kernel.org>
List-Subscribe: <mailto:bpf+subscribe@vger.kernel.org>
List-Unsubscribe: <mailto:bpf+unsubscribe@vger.kernel.org>
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101
 Thunderbird/102.3.0
Subject: Re: [PATCH bpf-next v2 3/3] bpf, riscv: use prog pack allocator in
 the BPF JIT
Content-Language: en-US
From: Pu Lehui <pulehui@huawei.com>
To: Puranjay Mohan <puranjay12@gmail.com>
CC: <bjorn@kernel.org>, <paul.walmsley@sifive.com>, <palmer@dabbelt.com>,
	<aou@eecs.berkeley.edu>, <conor.dooley@microchip.com>, <ast@kernel.org>,
	<daniel@iogearbox.net>, <andrii@kernel.org>, <martin.lau@linux.dev>,
	<song@kernel.org>, <yhs@fb.com>, <linux-riscv@lists.infradead.org>,
	<bpf@vger.kernel.org>, <kpsingh@kernel.org>, <linux-kernel@vger.kernel.org>
References: <20230824133135.1176709-1-puranjay12@gmail.com>
 <20230824133135.1176709-4-puranjay12@gmail.com>
 <3e21f79c-71a8-663e-1a62-0d2d787b9692@huawei.com>
In-Reply-To: <3e21f79c-71a8-663e-1a62-0d2d787b9692@huawei.com>
Content-Type: text/plain; charset="UTF-8"; format=flowed
Content-Transfer-Encoding: 8bit
X-Originating-IP: [10.67.109.184]
X-ClientProxiedBy: dggems702-chm.china.huawei.com (10.3.19.179) To
 kwepemi500020.china.huawei.com (7.221.188.8)
X-CFilter-Loop: Reflected
X-Spam-Status: No, score=-4.8 required=5.0 tests=BAYES_00,NICE_REPLY_A,
	RCVD_IN_DNSWL_BLOCKED,RCVD_IN_MSPIKE_H5,RCVD_IN_MSPIKE_WL,
	SPF_HELO_NONE,SPF_PASS autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
	lindbergh.monkeyblade.net



On 2023/8/25 15:09, Pu Lehui wrote:
> Hi Puranjay,
> 
> Happy to see the RV64 pack allocator implementation.

RV32 also

> 
> On 2023/8/24 21:31, Puranjay Mohan wrote:
>> Use bpf_jit_binary_pack_alloc() for memory management of JIT binaries in
>> RISCV BPF JIT. The bpf_jit_binary_pack_alloc creates a pair of RW and RX
>> buffers. The JIT writes the program into the RW buffer. When the JIT is
>> done, the program is copied to the final RX buffer with
>> bpf_jit_binary_pack_finalize.
>>
>> Implement bpf_arch_text_copy() and bpf_arch_text_invalidate() for RISCV
>> JIT as these functions are required by bpf_jit_binary_pack allocator.
>>
>> Signed-off-by: Puranjay Mohan <puranjay12@gmail.com>
>> ---
>> Â  arch/riscv/net/bpf_jit.hÂ Â Â Â Â Â Â  |Â Â  3 +
>> Â  arch/riscv/net/bpf_jit_comp64.c |Â  56 +++++++++++++---
>> Â  arch/riscv/net/bpf_jit_core.cÂ Â  | 113 +++++++++++++++++++++++++++-----
>> Â  3 files changed, 146 insertions(+), 26 deletions(-)
>>
>> diff --git a/arch/riscv/net/bpf_jit.h b/arch/riscv/net/bpf_jit.h
>> index 2717f5490428..ad69319c8ea7 100644
>> --- a/arch/riscv/net/bpf_jit.h
>> +++ b/arch/riscv/net/bpf_jit.h
>> @@ -68,6 +68,7 @@ static inline bool is_creg(u8 reg)
>> Â  struct rv_jit_context {
>> Â Â Â Â Â  struct bpf_prog *prog;
>> Â Â Â Â Â  u16 *insns;Â Â Â Â Â Â Â  /* RV insns */
>> +Â Â Â  u16 *ro_insns;

In fact, the definition of w/ or w/o ro_ still looks a bit confusing. 
Maybe it is better for us not to change the current framework, as the 
current `image` is the final executed RX image, and the trampoline 
treats `image` as the same. Maybe it would be better to add a new RW 
image, such like `rw_iamge`, so that we do not break the existing 
framework and do not have to add too many comments.

And any other parts, it looks great.ðŸ˜„

>> Â Â Â Â Â  int ninsns;
>> Â Â Â Â Â  int prologue_len;
>> Â Â Â Â Â  int epilogue_offset;
>> @@ -85,7 +86,9 @@ static inline int ninsns_rvoff(int ninsns)
>> Â  struct rv_jit_data {
>> Â Â Â Â Â  struct bpf_binary_header *header;
>> +Â Â Â  struct bpf_binary_header *ro_header;
>> Â Â Â Â Â  u8 *image;
>> +Â Â Â  u8 *ro_image;
>> Â Â Â Â Â  struct rv_jit_context ctx;
>> Â  };
>> diff --git a/arch/riscv/net/bpf_jit_comp64.c 
>> b/arch/riscv/net/bpf_jit_comp64.c
>> index 0ca4f5c0097c..d77b16338ba2 100644
>> --- a/arch/riscv/net/bpf_jit_comp64.c
>> +++ b/arch/riscv/net/bpf_jit_comp64.c
>> @@ -144,7 +144,11 @@ static bool in_auipc_jalr_range(s64 val)
>> Â  /* Emit fixed-length instructions for address */
>> Â  static int emit_addr(u8 rd, u64 addr, bool extra_pass, struct 
>> rv_jit_context *ctx)
>> Â  {
>> -Â Â Â  u64 ip = (u64)(ctx->insns + ctx->ninsns);
>> +Â Â Â  /*
>> +Â Â Â Â  * Use the ro_insns(RX) to calculate the offset as the BPF 
>> program will
>> +Â Â Â Â  * finally run from this memory region.
>> +Â Â Â Â  */
>> +Â Â Â  u64 ip = (u64)(ctx->ro_insns + ctx->ninsns);
>> Â Â Â Â Â  s64 off = addr - ip;
>> Â Â Â Â Â  s64 upper = (off + (1 << 11)) >> 12;
>> Â Â Â Â Â  s64 lower = off & 0xfff;
>> @@ -465,7 +469,11 @@ static int emit_call(u64 addr, bool fixed_addr, 
>> struct rv_jit_context *ctx)
>> Â Â Â Â Â  u64 ip;
>> Â Â Â Â Â  if (addr && ctx->insns) {
> 
> ctx->insns need to sync to ctx->ro_insns
> 
>> -Â Â Â Â Â Â Â  ip = (u64)(long)(ctx->insns + ctx->ninsns);
>> +Â Â Â Â Â Â Â  /*
>> +Â Â Â Â Â Â Â Â  * Use the ro_insns(RX) to calculate the offset as the BPF
>> +Â Â Â Â Â Â Â Â  * program will finally run from this memory region.
>> +Â Â Â Â Â Â Â Â  */
>> +Â Â Â Â Â Â Â  ip = (u64)(long)(ctx->ro_insns + ctx->ninsns);
>> Â Â Â Â Â Â Â Â Â  off = addr - ip;
>> Â Â Â Â Â  }
>> @@ -578,7 +586,8 @@ static int add_exception_handler(const struct 
>> bpf_insn *insn,
>> Â  {
>> Â Â Â Â Â  struct exception_table_entry *ex;
>> Â Â Â Â Â  unsigned long pc;
>> -Â Â Â  off_t offset;
>> +Â Â Â  off_t ins_offset;
>> +Â Â Â  off_t fixup_offset;
>> Â Â Â Â Â  if (!ctx->insns || !ctx->prog->aux->extable || 
>> BPF_MODE(insn->code) != BPF_PROBE_MEM)
> 
> ctx->ro_insns need to be checked also.
> 
>> Â Â Â Â Â Â Â Â Â  return 0;
>> @@ -593,12 +602,17 @@ static int add_exception_handler(const struct 
>> bpf_insn *insn,
>> Â Â Â Â Â Â Â Â Â  return -EINVAL;
>> Â Â Â Â Â  ex = &ctx->prog->aux->extable[ctx->nexentries];
>> -Â Â Â  pc = (unsigned long)&ctx->insns[ctx->ninsns - insn_len];
>> +Â Â Â  pc = (unsigned long)&ctx->ro_insns[ctx->ninsns - insn_len];
>> -Â Â Â  offset = pc - (long)&ex->insn;
>> -Â Â Â  if (WARN_ON_ONCE(offset >= 0 || offset < INT_MIN))
>> +Â Â Â  /*
>> +Â Â Â Â  * This is the relative offset of the instruction that may fault 
>> from
>> +Â Â Â Â  * the exception table itself. This will be written to the exception
>> +Â Â Â Â  * table and if this instruction faults, the destination register 
>> will
>> +Â Â Â Â  * be set to '0' and the execution will jump to the next 
>> instruction.
>> +Â Â Â Â  */
>> +Â Â Â  ins_offset = pc - (long)&ex->insn;
>> +Â Â Â  if (WARN_ON_ONCE(ins_offset >= 0 || ins_offset < INT_MIN))
>> Â Â Â Â Â Â Â Â Â  return -ERANGE;
>> -Â Â Â  ex->insn = offset;
>> Â Â Â Â Â  /*
>> Â Â Â Â Â Â  * Since the extable follows the program, the fixup offset is 
>> always
>> @@ -607,12 +621,25 @@ static int add_exception_handler(const struct 
>> bpf_insn *insn,
>> Â Â Â Â Â Â  * bits. We don't need to worry about buildtime or runtime sort
>> Â Â Â Â Â Â  * modifying the upper bits because the table is already sorted, 
>> and
>> Â Â Â Â Â Â  * isn't part of the main exception table.
>> +Â Â Â Â  *
>> +Â Â Â Â  * The fixup_offset is set to the next instruction from the 
>> instruction
>> +Â Â Â Â  * that may fault. The execution will jump to this after handling 
>> the
>> +Â Â Â Â  * fault.
>> Â Â Â Â Â Â  */
>> -Â Â Â  offset = (long)&ex->fixup - (pc + insn_len * sizeof(u16));
>> -Â Â Â  if (!FIELD_FIT(BPF_FIXUP_OFFSET_MASK, offset))
>> +Â Â Â  fixup_offset = (long)&ex->fixup - (pc + insn_len * sizeof(u16));
>> +Â Â Â  if (!FIELD_FIT(BPF_FIXUP_OFFSET_MASK, fixup_offset))
>> Â Â Â Â Â Â Â Â Â  return -ERANGE;
>> -Â Â Â  ex->fixup = FIELD_PREP(BPF_FIXUP_OFFSET_MASK, offset) |
>> +Â Â Â  /*
>> +Â Â Â Â  * The offsets above have been calculated using the RO buffer but we
>> +Â Â Â Â  * need to use the R/W buffer for writes.
>> +Â Â Â Â  * switch ex to rw buffer for writing.
>> +Â Â Â Â  */
>> +Â Â Â  ex = (void *)ctx->insns + ((void *)ex - (void *)ctx->ro_insns);
>> +
>> +Â Â Â  ex->insn = ins_offset;
>> +
>> +Â Â Â  ex->fixup = FIELD_PREP(BPF_FIXUP_OFFSET_MASK, fixup_offset) |
>> Â Â Â Â Â Â Â Â Â  FIELD_PREP(BPF_FIXUP_REG_MASK, dst_reg);
>> Â Â Â Â Â  ex->type = EX_TYPE_BPF;
>> @@ -1006,6 +1033,7 @@ int arch_prepare_bpf_trampoline(struct 
>> bpf_tramp_image *im, void *image,
>> Â Â Â Â Â  ctx.ninsns = 0;
>> Â Â Â Â Â  ctx.insns = NULL;
>> +Â Â Â  ctx.ro_insns = NULL;
>> Â Â Â Â Â  ret = __arch_prepare_bpf_trampoline(im, m, tlinks, func_addr, 
>> flags, &ctx);
>> Â Â Â Â Â  if (ret < 0)
>> Â Â Â Â Â Â Â Â Â  return ret;
>> @@ -1014,7 +1042,15 @@ int arch_prepare_bpf_trampoline(struct 
>> bpf_tramp_image *im, void *image,
>> Â Â Â Â Â Â Â Â Â  return -EFBIG;
>> Â Â Â Â Â  ctx.ninsns = 0;
>> +Â Â Â  /*
>> +Â Â Â Â  * The bpf_int_jit_compile() uses a RW buffer (ctx.insns) to 
>> write the
>> +Â Â Â Â  * JITed instructions and later copies it to a RX region 
>> (ctx.ro_insns).
>> +Â Â Â Â  * It also uses ctx.ro_insns to calculate offsets for jumps etc. 
>> As the
>> +Â Â Â Â  * trampoline image uses the same memory area for writing and 
>> execution,
>> +Â Â Â Â  * both ctx.insns and ctx.ro_insns can be set to image.
>> +Â Â Â Â  */
>> Â Â Â Â Â  ctx.insns = image;
>> +Â Â Â  ctx.ro_insns = image;
>> Â Â Â Â Â  ret = __arch_prepare_bpf_trampoline(im, m, tlinks, func_addr, 
>> flags, &ctx);
>> Â Â Â Â Â  if (ret < 0)
>> Â Â Â Â Â Â Â Â Â  return ret;
>> diff --git a/arch/riscv/net/bpf_jit_core.c 
>> b/arch/riscv/net/bpf_jit_core.c
>> index 7a26a3e1c73c..4c8dffc09368 100644
>> --- a/arch/riscv/net/bpf_jit_core.c
>> +++ b/arch/riscv/net/bpf_jit_core.c
>> @@ -8,6 +8,8 @@
>> Â  #include <linux/bpf.h>
>> Â  #include <linux/filter.h>
>> +#include <linux/memory.h>
>> +#include <asm/patch.h>
>> Â  #include "bpf_jit.h"
>> Â  /* Number of iterations to try until offsets converge. */
>> @@ -117,16 +119,27 @@ struct bpf_prog *bpf_int_jit_compile(struct 
>> bpf_prog *prog)
>> Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  sizeof(struct exception_table_entry);
>> Â Â Â Â Â Â Â Â Â Â Â Â Â  prog_size = sizeof(*ctx->insns) * ctx->ninsns;
>> -Â Â Â Â Â Â Â Â Â Â Â  jit_data->header =
>> -Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  bpf_jit_binary_alloc(prog_size + extable_size,
>> -Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  &jit_data->image,
>> -Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  sizeof(u32),
>> -Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  bpf_fill_ill_insns);
>> -Â Â Â Â Â Â Â Â Â Â Â  if (!jit_data->header) {
>> +Â Â Â Â Â Â Â Â Â Â Â  jit_data->ro_header =
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  bpf_jit_binary_pack_alloc(prog_size +
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  extable_size,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  &jit_data->ro_image,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  sizeof(u32),
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  &jit_data->header,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  &jit_data->image,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  bpf_fill_ill_insns);
>> +Â Â Â Â Â Â Â Â Â Â Â  if (!jit_data->ro_header) {
>> Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  prog = orig_prog;
>> Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  goto out_offset;
>> Â Â Â Â Â Â Â Â Â Â Â Â Â  }
>> +Â Â Â Â Â Â Â Â Â Â Â  /*
>> +Â Â Â Â Â Â Â Â Â Â Â Â  * Use the image(RW) for writing the JITed instructions. 
>> But also save
>> +Â Â Â Â Â Â Â Â Â Â Â Â  * the ro_image(RX) for calculating the offsets in the 
>> image. The RW
>> +Â Â Â Â Â Â Â Â Â Â Â Â  * image will be later copied to the RX image from where 
>> the program
>> +Â Â Â Â Â Â Â Â Â Â Â Â  * will run. The bpf_jit_binary_pack_finalize() will do 
>> this copy in the
>> +Â Â Â Â Â Â Â Â Â Â Â Â  * final step.
>> +Â Â Â Â Â Â Â Â Â Â Â Â  */
>> +Â Â Â Â Â Â Â Â Â Â Â  ctx->ro_insns = (u16 *)jit_data->ro_image;
>> Â Â Â Â Â Â Â Â Â Â Â Â Â  ctx->insns = (u16 *)jit_data->image;
>> Â Â Â Â Â Â Â Â Â Â Â Â Â  /*
>> Â Â Â Â Â Â Â Â Â Â Â Â Â Â  * Now, when the image is allocated, the image can
>> @@ -138,14 +151,12 @@ struct bpf_prog *bpf_int_jit_compile(struct 
>> bpf_prog *prog)
>> Â Â Â Â Â  if (i == NR_JIT_ITERATIONS) {
>> Â Â Â Â Â Â Â Â Â  pr_err("bpf-jit: image did not converge in <%d passes!\n", i);
>> -Â Â Â Â Â Â Â  if (jit_data->header)
>> -Â Â Â Â Â Â Â Â Â Â Â  bpf_jit_binary_free(jit_data->header);
>> Â Â Â Â Â Â Â Â Â  prog = orig_prog;
>> -Â Â Â Â Â Â Â  goto out_offset;
>> +Â Â Â Â Â Â Â  goto out_free_hdr;
>> Â Â Â Â Â  }
>> Â Â Â Â Â  if (extable_size)
>> -Â Â Â Â Â Â Â  prog->aux->extable = (void *)ctx->insns + prog_size;
>> +Â Â Â Â Â Â Â  prog->aux->extable = (void *)ctx->ro_insns + prog_size;
>> Â  skip_init_ctx:
>> Â Â Â Â Â  pass++;
>> @@ -154,23 +165,35 @@ struct bpf_prog *bpf_int_jit_compile(struct 
>> bpf_prog *prog)
>> Â Â Â Â Â  bpf_jit_build_prologue(ctx);
>> Â Â Â Â Â  if (build_body(ctx, extra_pass, NULL)) {
>> -Â Â Â Â Â Â Â  bpf_jit_binary_free(jit_data->header);
>> Â Â Â Â Â Â Â Â Â  prog = orig_prog;
>> -Â Â Â Â Â Â Â  goto out_offset;
>> +Â Â Â Â Â Â Â  goto out_free_hdr;
>> Â Â Â Â Â  }
>> Â Â Â Â Â  bpf_jit_build_epilogue(ctx);
>> Â Â Â Â Â  if (bpf_jit_enable > 1)
>> Â Â Â Â Â Â Â Â Â  bpf_jit_dump(prog->len, prog_size, pass, ctx->insns);
>> -Â Â Â  prog->bpf_func = (void *)ctx->insns;
>> +Â Â Â  prog->bpf_func = (void *)ctx->ro_insns;
>> Â Â Â Â Â  prog->jited = 1;
>> Â Â Â Â Â  prog->jited_len = prog_size;
>> -Â Â Â  bpf_flush_icache(jit_data->header, ctx->insns + ctx->ninsns);
>> -
>> Â Â Â Â Â  if (!prog->is_func || extra_pass) {
>> -Â Â Â Â Â Â Â  bpf_jit_binary_lock_ro(jit_data->header);
>> +Â Â Â Â Â Â Â  if (WARN_ON(bpf_jit_binary_pack_finalize(prog,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  jit_data->ro_header,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  jit_data->header))) {
>> +Â Â Â Â Â Â Â Â Â Â Â  /* ro_header has been freed */
>> +Â Â Â Â Â Â Â Â Â Â Â  jit_data->ro_header = NULL;
>> +Â Â Â Â Â Â Â Â Â Â Â  prog = orig_prog;
>> +Â Â Â Â Â Â Â Â Â Â Â  goto out_offset;
>> +Â Â Â Â Â Â Â  }
>> +Â Â Â Â Â Â Â  /*
>> +Â Â Â Â Â Â Â Â  * The instructions have now been copied to the ROX region from
>> +Â Â Â Â Â Â Â Â  * where they will execute.
>> +Â Â Â Â Â Â Â Â  * Write any modified data cache blocks out to memory and
>> +Â Â Â Â Â Â Â Â  * invalidate the corresponding blocks in the instruction cache.
>> +Â Â Â Â Â Â Â Â  */
>> +Â Â Â Â Â Â Â  bpf_flush_icache(jit_data->ro_header,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ctx->ro_insns + ctx->ninsns);
>> Â Â Â Â Â Â Â Â Â  for (i = 0; i < prog->len; i++)
>> Â Â Â Â Â Â Â Â Â Â Â Â Â  ctx->offset[i] = ninsns_rvoff(ctx->offset[i]);
>> Â Â Â Â Â Â Â Â Â  bpf_prog_fill_jited_linfo(prog, ctx->offset);
>> @@ -185,6 +208,15 @@ struct bpf_prog *bpf_int_jit_compile(struct 
>> bpf_prog *prog)
>> Â Â Â Â Â Â Â Â Â  bpf_jit_prog_release_other(prog, prog == orig_prog ?
>> Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  tmp : orig_prog);
>> Â Â Â Â Â  return prog;
>> +
>> +out_free_hdr:
>> +Â Â Â  if (jit_data->header) {
>> +Â Â Â Â Â Â Â  bpf_arch_text_copy(&jit_data->ro_header->size,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  &jit_data->header->size,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  sizeof(jit_data->header->size));
>> +Â Â Â Â Â Â Â  bpf_jit_binary_pack_free(jit_data->ro_header, jit_data->header);
>> +Â Â Â  }
>> +Â Â Â  goto out_offset;
>> Â  }
>> Â  u64 bpf_jit_alloc_exec_limit(void)
>> @@ -204,3 +236,52 @@ void bpf_jit_free_exec(void *addr)
>> Â  {
>> Â Â Â Â Â  return vfree(addr);
>> Â  }
>> +
>> +void *bpf_arch_text_copy(void *dst, void *src, size_t len)
>> +{
>> +Â Â Â  int ret;
>> +
>> +Â Â Â  mutex_lock(&text_mutex);
>> +Â Â Â  ret = patch_text_nosync(dst, src, len);
>> +Â Â Â  mutex_unlock(&text_mutex);
>> +
>> +Â Â Â  if (ret)
>> +Â Â Â Â Â Â Â  return ERR_PTR(-EINVAL);
>> +
>> +Â Â Â  return dst;
>> +}
>> +
>> +int bpf_arch_text_invalidate(void *dst, size_t len)
>> +{
>> +Â Â Â  int ret = 0;
> 
> no need to initialize it
> 
>> +
>> +Â Â Â  mutex_lock(&text_mutex);
>> +Â Â Â  ret = patch_text_set_nosync(dst, 0, len);
>> +Â Â Â  mutex_unlock(&text_mutex);
>> +
>> +Â Â Â  return ret;
>> +}
>> +
>> +void bpf_jit_free(struct bpf_prog *prog)
>> +{
>> +Â Â Â  if (prog->jited) {
>> +Â Â Â Â Â Â Â  struct rv_jit_data *jit_data = prog->aux->jit_data;
>> +Â Â Â Â Â Â Â  struct bpf_binary_header *hdr;
>> +
>> +Â Â Â Â Â Â Â  /*
>> +Â Â Â Â Â Â Â Â  * If we fail the final pass of JIT (from jit_subprogs),
>> +Â Â Â Â Â Â Â Â  * the program may not be finalized yet. Call finalize here
>> +Â Â Â Â Â Â Â Â  * before freeing it.
>> +Â Â Â Â Â Â Â Â  */
>> +Â Â Â Â Â Â Â  if (jit_data) {
>> +Â Â Â Â Â Â Â Â Â Â Â  bpf_jit_binary_pack_finalize(prog, jit_data->ro_header,
>> +Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  jit_data->header);
>> +Â Â Â Â Â Â Â Â Â Â Â  kfree(jit_data);
>> +Â Â Â Â Â Â Â  }
>> +Â Â Â Â Â Â Â  hdr = bpf_jit_binary_pack_hdr(prog);
>> +Â Â Â Â Â Â Â  bpf_jit_binary_pack_free(hdr, NULL);
>> +Â Â Â Â Â Â Â  WARN_ON_ONCE(!bpf_prog_kallsyms_verify_off(prog));
>> +Â Â Â  }
>> +
>> +Â Â Â  bpf_prog_unlock_free(prog);
>> +}
> 
> 

